{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_17648\\1786680353.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['topics'][i] = df['topics'][i].replace(\"'',\", \"\")\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "df = pd.read_csv('./results4.csv')\n",
    "df.columns = ['id', 'text', 'modText', 'sentiment', 'topics', 'hashtags']\n",
    "\n",
    "negatives = ['not', 'no', 'nor', \"won't\", \"wouldn't\", \"shouldn't\", \"couldn't\", 'against', \"aren't\", \"haven't\", \"hasn't\", \"doesn't\", \"isn't\", \"don't\"]\n",
    "\n",
    "#tokenize text and lowercase\n",
    "df['tokText'] = df['text'].apply(lambda x: x.split())\n",
    "df['tokText'] = df['tokText'].apply(lambda x: [y.lower() for y in x])\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    df['topics'][i] = df['topics'][i].replace(\"'',\", \"\")\n",
    "df['topics'] = df['topics'].apply(lambda x: x[1:-1].split(', '))\n",
    "\n",
    "selectedIndex = []\n",
    "#if text has 2 or more negative words, add index to selectedIndex\n",
    "for i in range(0, len(df)):\n",
    "    count = 0\n",
    "    for word in df['tokText'][i]:\n",
    "        if word in negatives:\n",
    "            count += 1\n",
    "    if count >= 2:\n",
    "        selectedIndex.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\GitHub\\Sntiment-Analysis-from-Tweets\\DoubleNegativeAnalysis.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W1sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m userDF[\u001b[39m'\u001b[39m\u001b[39mtopicsRaw\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m userDF[\u001b[39m'\u001b[39m\u001b[39mtopicsRaw\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W1sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#using soft cosine similarity return 10 most similar users to a given user\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W1sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m fasttext_model300 \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mfasttext-wiki-news-subwords-300\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W1sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m documents \u001b[39m=\u001b[39m userDF[\u001b[39m'\u001b[39m\u001b[39mtopics\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W1sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m dictionary \u001b[39m=\u001b[39m Dictionary(documents)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    501\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, BASE_DIR)\n\u001b[0;32m    502\u001b[0m module \u001b[39m=\u001b[39m \u001b[39m__import__\u001b[39m(name)\n\u001b[1;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49mload_data()\n",
      "File \u001b[1;32m~/gensim-data\\fasttext-wiki-news-subwords-300\\__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m():\n\u001b[0;32m      7\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m'\u001b[39m\u001b[39mfasttext-wiki-news-subwords-300\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfasttext-wiki-news-subwords-300.gz\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(path, binary\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m     \u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[0;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[0;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[0;32m   1722\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[0;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[0;32m   2067\u001b[0m         )\n\u001b[0;32m   2068\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2069\u001b[0m         _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[0;32m   2070\u001b[0m \u001b[39mif\u001b[39;00m kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(kv):\n\u001b[0;32m   2071\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   2072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2073\u001b[0m         kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(kv),\n\u001b[0;32m   2074\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gensim\\models\\keyedvectors.py:1971\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m   1969\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word2vec_read_text\u001b[39m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding):\n\u001b[0;32m   1970\u001b[0m     \u001b[39mfor\u001b[39;00m line_no \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vocab_size):\n\u001b[1;32m-> 1971\u001b[0m         line \u001b[39m=\u001b[39m fin\u001b[39m.\u001b[39;49mreadline()\n\u001b[0;32m   1972\u001b[0m         \u001b[39mif\u001b[39;00m line \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   1973\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\gzip.py:399\u001b[0m, in \u001b[0;36mGzipFile.readline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadline\u001b[39m(\u001b[39mself\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    398\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_not_closed()\n\u001b[1;32m--> 399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mreadline(size)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(data)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\gzip.py:496\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[39m# Read a chunk of data from the file\u001b[39;00m\n\u001b[0;32m    494\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread(io\u001b[39m.\u001b[39mDEFAULT_BUFFER_SIZE)\n\u001b[1;32m--> 496\u001b[0m uncompress \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decompressor\u001b[39m.\u001b[39;49mdecompress(buf, size)\n\u001b[0;32m    497\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail \u001b[39m!=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    498\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mprepend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decompressor\u001b[39m.\u001b[39munconsumed_tail)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "\n",
    "userDF = pd.DataFrame(columns=['id', 'topics',])\n",
    "userDF.id = df.id.unique()\n",
    "#add all unique topics to userDF by id unless it is empty\n",
    "for i in range(0, len(userDF)):\n",
    "    userDF['topics'][i] = df[df['id'] == userDF['id'][i]]['topics'].sum()\n",
    "    userDF['topics'][i] = [x for x in userDF['topics'][i] if x != \"\\'\\'\"]\n",
    "    \n",
    "userDF['topicsRaw'] = userDF['topics'].copy()\n",
    "\n",
    "#change sentiment 4 to 1\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 4 else x)\n",
    "\n",
    "#for each topic add sentiment based on mean of sentiment of the user's tweets with that topic\n",
    "for i in range(0, len(userDF)):\n",
    "    new_row = []\n",
    "\n",
    "    for j in range(0, len(userDF['topics'][i])):\n",
    "        new_row.append(userDF['topics'][i][j] + str((df[(df['id'] == userDF['id'][i]) & (df['topics'].apply(lambda x: userDF['topics'][i][j] in x))]['sentiment'].mean()).round(2)))\n",
    "        \n",
    "    userDF.at[i, 'topics'] = new_row\n",
    "\n",
    "#convert array of topicsRaw to string\n",
    "userDF['topicsRaw'] = userDF['topicsRaw'].apply(lambda x: str(x))\n",
    "\n",
    "#clean topicsRaw and convert to array\n",
    "userDF['topicsRaw'] = userDF['topicsRaw'].apply(lambda x: x.replace(\"\\\"\", \"\"))\n",
    "userDF['topicsRaw'] = userDF['topicsRaw'].apply(lambda x: x[1:-1].split(', '))\n",
    "\n",
    "#using soft cosine similarity return 10 most similar users to a given user\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "documents = userDF['topics']\n",
    "dictionary = Dictionary(documents)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "index = WordEmbeddingSimilarityIndex(fasttext_model300)\n",
    "sims = SparseTermSimilarityMatrix(index, dictionary)\n",
    "model = SoftCosineSimilarity(bow_corpus, sims, num_best=10)\n",
    "\n",
    "def get_recommendations(id):\n",
    "    idx = userDF.index[userDF['id'] == id][0]\n",
    "    query = userDF['topics'][idx]\n",
    "    query = dictionary.doc2bow(query)\n",
    "    return model[query]\n",
    "\n",
    "dfCopy = df.copy()\n",
    "userDfCopy = userDF.copy()\n",
    "\n",
    "predResults = []\n",
    "\n",
    "for i in range(0, len(selectedIndex)):\n",
    "    \n",
    "    df = dfCopy.copy()\n",
    "    userDfCopy = userDF.copy()\n",
    "    \n",
    "    #get closest users to the given user to predict sentiment towards a topic\n",
    "    results = get_recommendations(df['id'][selectedIndex[i]])\n",
    "\n",
    "    #only use users with a distance of less than 0.3\n",
    "    results = [x for x in results if x[1] < 0.3 ]\n",
    "\n",
    "    #assign user{number} to each user\n",
    "    userDF['user'] = [\"user\" + str(i) for i in range(0, len(userDF))]\n",
    "    userDF['userNum'] = [str(i) for i in range(0, len(userDF))]\n",
    "\n",
    "    print(\"Closest users to\", userDF.loc[userDF['id'] == df['id'][selectedIndex[i]], 'user'].iloc[0], \"are:\", [userDF['user'][x[0]] for x in results], \"with distances of:\", [x[1] for x in results])\n",
    "\n",
    "    #predict sentiment towards the chosen text based on the closest users' sentiment towards the topic\n",
    "    topics = df['topics'][selectedIndex[i]]\n",
    "\n",
    "    #find texts from close users with at least 1 topic in common with the chosen text and add to df\n",
    "    closeDF = pd.DataFrame(columns=['id', 'text', 'modText', 'sentiment', 'topics', 'hashtags'])\n",
    "    for i in range(0, len(results)):\n",
    "        closeDF = closeDF.append(df[(df['id'] == userDF['id'][results[i][0]]) & (df['topics'].apply(lambda x: any(item in x for item in topics)))])\n",
    "\n",
    "    #perform knn on the close users' sentiment towards the topic\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    closeDF = closeDF.append(pd.Series(), ignore_index=True)\n",
    "    chosenTopics = df['topics'][selectedIndex[i]]\n",
    "    closeDF['topics'].iloc[-1] = chosenTopics\n",
    "\n",
    "    closeDF['topics'] = closeDF['topics'].astype(str)\n",
    "    closeDF['topics'] = closeDF['topics'].astype(pd.StringDtype())\n",
    "    for i in range(0, len(closeDF)):\n",
    "        closeDF['topics'].iloc[i] = closeDF['topics'].iloc[i].replace(\"'\", \"\")\n",
    "        closeDF['topics'].iloc[i] = closeDF['topics'].iloc[i].replace(\",\", \"\")\n",
    "        closeDF['topics'].iloc[i] = closeDF['topics'].iloc[i].replace(\"[\", \"\")\n",
    "        closeDF['topics'].iloc[i] = closeDF['topics'].iloc[i].replace(\"]\", \"\")\n",
    "        closeDF['topics'].iloc[i] = closeDF['topics'].iloc[i].replace(\"\\\"\", \"\")\n",
    "\n",
    "    #encode topics as bag of words\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "        X = vectorizer.fit_transform(closeDF['topics'].astype(str))\n",
    "        X = pd.DataFrame(X.todense())\n",
    "\n",
    "        #train knn\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(X[:-1].astype(int), closeDF['sentiment'][:-1].astype(int))\n",
    "\n",
    "        #predict sentiment towards the chosen text\n",
    "        closeDF['sentiment'].iloc[-1] = knn.predict(X[-1:])\n",
    "        \n",
    "        predResults.append(closeDF['sentiment'].iloc[-1])\n",
    "    except:\n",
    "        predResults.append(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_17648\\979792583.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  train = train.append(pd.Series(), ignore_index=True)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_17648\\979792583.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train = train.append(pd.Series(), ignore_index=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predResults' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\GitHub\\Sntiment-Analysis-from-Tweets\\DoubleNegativeAnalysis.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train, test \u001b[39m=\u001b[39m train_test_split(df, test_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mappend(pd\u001b[39m.\u001b[39mSeries(), ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m predResults\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#train knn on the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mneighbors\u001b[39;00m \u001b[39mimport\u001b[39;00m KNeighborsClassifier\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predResults' is not defined"
     ]
    }
   ],
   "source": [
    "#split df into train and test. test on same percentage as the number of selected texts. \n",
    "\n",
    "testSize = len(selectedIndex)/len(df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=100, random_state=42)\n",
    "\n",
    "train = train.append(pd.Series(), ignore_index=True)\n",
    "train['sentiment'].iloc[-1] = predResults\n",
    "\n",
    "#train knn on the dataset\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train['topics'] = train['topics'].astype(str)\n",
    "train['topics'] = train['topics'].astype(pd.StringDtype())\n",
    "for i in range(0, len(train)):\n",
    "    train['topics'].iloc[i] = train['topics'].iloc[i].replace(\"'\", \"\")\n",
    "    train['topics'].iloc[i] = train['topics'].iloc[i].replace(\",\", \"\")\n",
    "    train['topics'].iloc[i] = train['topics'].iloc[i].replace(\"[\", \"\")\n",
    "    train['topics'].iloc[i] = train['topics'].iloc[i].replace(\"]\", \"\")\n",
    "    train['topics'].iloc[i] = train['topics'].iloc[i].replace(\"\\\"\", \"\")\n",
    "    \n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "X = vectorizer.fit_transform(train['topics'].astype(str))\n",
    "X = pd.DataFrame(X.todense())\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X[:-1].astype(int), train['sentiment'][:-1].astype(int))\n",
    "\n",
    "#predict sentiment towards the test texts\n",
    "test['topics'] = test['topics'].astype(str)\n",
    "test['topics'] = test['topics'].astype(pd.StringDtype())\n",
    "for i in range(0, len(test)):\n",
    "    test['topics'].iloc[i] = test['topics'].iloc[i].replace(\"'\", \"\")\n",
    "    test['topics'].iloc[i] = test['topics'].iloc[i].replace(\",\", \"\")\n",
    "    test['topics'].iloc[i] = test['topics'].iloc[i].replace(\"[\", \"\")\n",
    "    test['topics'].iloc[i] = test['topics'].iloc[i].replace(\"]\", \"\")\n",
    "    test['topics'].iloc[i] = test['topics'].iloc[i].replace(\"\\\"\", \"\")\n",
    "    \n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "X = vectorizer.fit_transform(test['topics'].astype(str))\n",
    "X = pd.DataFrame(X.todense())\n",
    "\n",
    "a = knn.predict(X)\n",
    "\n",
    "#evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(test['sentiment'], a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([0]), 9, 9, array([1]), 9, array([0]), array([0]), array([1]), 9, array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([1]), 9, 9, 9, array([0]), 9, array([0]), array([0]), 9, array([1]), array([1]), 9, array([0]), array([0]), array([0]), array([0]), array([0]), 9, array([0]), array([0]), 9, array([1]), array([1]), 9, 9, array([0]), array([0]), 9, array([0]), array([0]), array([0]), array([1]), array([0]), array([0]), 9, array([0]), array([0]), 9, array([0]), array([0]), 9, array([0]), 9, 9, array([1]), array([0]), array([0]), array([1]), array([0]), array([0]), array([0]), array([0]), array([0])]\n"
     ]
    }
   ],
   "source": [
    "print(predResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add original sentiment and predicted sentiment to new dataframe\n",
    "\n",
    "predDf = pd.DataFrame(columns=['id', 'ogSentiment', 'predSentiment'])\n",
    "predDf['id'] = df['id'][selectedIndex]\n",
    "predDf['ogSentiment'] = df['sentiment'][selectedIndex]\n",
    "predDf['predSentiment'] = predResults\n",
    "\n",
    "#remove all rows with predicted sentiment of 9\n",
    "predDf = predDf[predDf['predSentiment'] != 9]\n",
    "\n",
    "predDf.predSentiment = predDf.predSentiment.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 32\n",
      "Incorrect predictions: 24\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualise amount of correct predictions vs incorrect predictions\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for i in range(0, len(predDf)):\n",
    "    if predDf['ogSentiment'].iloc[i] == predDf['predSentiment'].iloc[i]:\n",
    "        correct += 1\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        \n",
    "print(\"Correct predictions:\", correct)\n",
    "print(\"Incorrect predictions:\", incorrect)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = 'Correct', 'Incorrect'\n",
    "sizes = [correct, incorrect]\n",
    "explode = (0, 0.1)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False, startangle=90)\n",
    "ax1.axis('equal')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\GitHub\\Sntiment-Analysis-from-Tweets\\DoubleNegativeAnalysis.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPredicted sentiment towards\u001b[39m\u001b[39m\"\u001b[39m, df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][selectedIndex], \u001b[39m\"\u001b[39m\u001b[39mis\u001b[39m\u001b[39m\"\u001b[39m, closeDF[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DoubleNegativeAnalysis.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOriginal sentiment towards\u001b[39m\u001b[39m\"\u001b[39m, df[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m][selectedIndex], \u001b[39m\"\u001b[39m\u001b[39mis\u001b[39m\u001b[39m\"\u001b[39m, df[\u001b[39m'\u001b[39m\u001b[39msentiment\u001b[39m\u001b[39m'\u001b[39m][selectedIndex])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Predicted sentiment towards\", df['text'][selectedIndex], \"is\", closeDF['sentiment'].iloc[-1])\n",
    "print(\"Original sentiment towards\", df['text'][selectedIndex], \"is\", df['sentiment'][selectedIndex])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
