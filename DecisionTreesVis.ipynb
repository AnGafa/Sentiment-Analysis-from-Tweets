{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22364\\3379405994.py:71: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22364\\3379405994.py:73: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_22364\\3379405994.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(stem_sentences)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train_original = pd.read_csv('./TrainingData/trainingdata2.csv')\n",
    "train_original.columns = ['target','id','date','flag','user','text']\n",
    "\n",
    "train=train_original[['id','text', 'target']]\n",
    "\n",
    "del train_original\n",
    " \n",
    "#region prepare stopwords list\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#remove useful words from the stopword list\n",
    "sw.remove('not')\n",
    "sw.remove('no')\n",
    "sw.remove('nor')\n",
    "sw.remove(\"won't\")\n",
    "sw.remove(\"wouldn't\")\n",
    "sw.remove(\"shouldn't\")\n",
    "sw.remove(\"couldn't\")\n",
    "sw.remove('against')\n",
    "sw.remove(\"aren't\")\n",
    "sw.remove(\"haven't\")\n",
    "sw.remove(\"hasn't\")\n",
    "sw.remove(\"doesn't\")\n",
    "sw.remove(\"isn't\")\n",
    "#endregion\n",
    "\n",
    "def remove_pattern(text,pattern):\n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    #tokenize the sentence and remove the stems of the words\n",
    "    ps = PorterStemmer()\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def preprocessTweet(df, sw):\n",
    "    #remove newlines\n",
    "    df['text'] = df['text'].str.replace(\"\\n\",\" \")\n",
    "    #turn all text to lowercase\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # remove twitter handles (@user)\n",
    "    df['text'] = np.vectorize(remove_pattern)(df['text'], \"@[\\w]*\")\n",
    "    #remove links\n",
    "    df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    #remove special characters, numbers, punctuations\n",
    "    df['text'] = df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    #remove short words (length < 3)\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([w for w in x.split() if (len(w)>3 or w == 'no')]))\n",
    "    #remove duplicate tweets - bot prevention\n",
    "    df['text'] = df['text'].drop_duplicates(keep=False)\n",
    "    #remove quotes\n",
    "    df['text'] = df['text'].str.replace(\"quot\", \"\")\n",
    "    #remove NANs\n",
    "    df.dropna(inplace=True)\n",
    "    #remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))\n",
    "    #remove empty tweets\n",
    "    df = df[df.text != '']\n",
    "    #stemming\n",
    "    df['text'] = df['text'].apply(stem_sentences)\n",
    "    return df\n",
    "\n",
    "train = preprocessTweet(train, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6682434244261809\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(train['text'])\n",
    "df_bow = pd.DataFrame(bow.todense())\n",
    "\n",
    "train_bow = bow[:]\n",
    "train_bow.todense()\n",
    "\n",
    "x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['target'],test_size=0.3,random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(x_train_bow, y_train_bow)\n",
    "y_pred = clf.predict(x_valid_bow)\n",
    "\n",
    "acc=accuracy_score(y_valid_bow,y_pred)\n",
    "print(acc)\n",
    "\n",
    "#save to csv\n",
    "resultsdf = pd.DataFrame({'text': x_valid_bow, 'pred Sentiment': y_pred})\n",
    "resultsdf.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677941543136987\n"
     ]
    }
   ],
   "source": [
    "tfidf = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(train['text'])\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "train_tfidf_matrix = tfidf_matrix[:]\n",
    "train_tfidf_matrix.todense()\n",
    "\n",
    "x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['target'],test_size=0.3,random_state=42)\n",
    "\n",
    "dct = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dct.fit(x_train_tfidf,y_train_tfidf)\n",
    "dct_tfidf = dct.predict(x_valid_tfidf)\n",
    "\n",
    "acc2=accuracy_score(y_valid_tfidf,dct_tfidf)\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#save models\n",
    "with open('DTBOWmodel.pkl','wb') as f:\n",
    "    pickle.dump(clf,f)\n",
    "    \n",
    "with open('DTTFIDFmodel.pkl','wb') as f:\n",
    "    pickle.dump(dct,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\GitHub\\Sntiment-Analysis-from-Tweets\\DecisionTreesVis.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#load models\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDTBOWmodel.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     clf \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDTTFIDFmodel.pkl\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     dct \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "#load models\n",
    "with open('DTBOWmodel.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "    \n",
    "with open('DTTFIDFmodel.pkl','rb') as f:\n",
    "    dct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 60000 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n60000 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_data.py\", line 809, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_data.py\", line 872, in partial_fit\n    raise ValueError(\nValueError: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Documents\\GitHub\\Sntiment-Analysis-from-Tweets\\DecisionTreesVis.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m X_train \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mfit_transform(x_train_bow)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m clf_GS \u001b[39m=\u001b[39m GridSearchCV(pipe, parameters)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m clf_GS\u001b[39m.\u001b[39;49mfit(X_train, y_train_bow)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbest Criterion:\u001b[39m\u001b[39m'\u001b[39m, clf_GS\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mget_params()[\u001b[39m'\u001b[39m\u001b[39mclf__criterion\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Documents/GitHub/Sntiment-Analysis-from-Tweets/DecisionTreesVis.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest max_depth:\u001b[39m\u001b[39m'\u001b[39m, clf_GS\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mget_params()[\u001b[39m'\u001b[39m\u001b[39mclf__max_depth\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    846\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    849\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    850\u001b[0m     )\n\u001b[1;32m--> 852\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    854\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 60000 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n60000 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 378, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 336, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\memory.py\", line 349, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\pipeline.py\", line 870, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 870, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_data.py\", line 809, in fit\n    return self.partial_fit(X, y, sample_weight)\n  File \"C:\\Users\\andre\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\preprocessing\\_data.py\", line 872, in partial_fit\n    raise ValueError(\nValueError: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "std_slc = StandardScaler()\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),('pca', pca), ('clf', clf)])\n",
    "\n",
    "X = train_bow\n",
    "y = train['target']\n",
    "\n",
    "n_components = list(range(1,X.shape[1]+1,1))\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [2,4,6,8,10,12]\n",
    "parameters = dict(pca__n_components=n_components, clf__criterion=criterion, clf__max_depth=max_depth)\n",
    "\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train = sc.fit_transform(x_train_bow)\n",
    "\n",
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X_train, y_train_bow)\n",
    "\n",
    "print('best Criterion:', clf_GS.best_estimator_.get_params()['clf__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['clf__max_depth'])\n",
    "print('Best no of components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n",
    "print()\n",
    "print(clf_GS.best_estimator_.get_params()['clf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "trainList = []\n",
    "validList = []\n",
    "\n",
    "for max_d in range(1,30):\n",
    "  model = tree.DecisionTreeClassifier(max_depth=max_d, random_state=42)\n",
    "  model.fit(x_train_bow, y_train_bow)\n",
    "  trainList.append(model.score(x_train_bow, y_train_bow))\n",
    "  validList.append(model.score(x_valid_bow, y_valid_bow))\n",
    "  print(len(trainList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6259061848734752\n",
      "0.6360614852345515\n"
     ]
    }
   ],
   "source": [
    "print(max(validList))\n",
    "print(max(trainList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "624741679a3ae5d99cecf49b8df5d516a7a937e6e7328e129d1fa121c8592e26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
