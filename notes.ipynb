{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing stop words for NN model might be harmful - compare results\n",
    "\n",
    "removing negative stopwords from list\n",
    "\n",
    "bot message analysis\n",
    "\n",
    "literature review - table:\n",
    "\n",
    "#, author, alogorithm, dataset, result\n",
    "1 ..\n",
    "2 .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic extraction - hashtags\n",
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_16440\\3379405994.py:71: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_16440\\3379405994.py:73: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text'] = df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_16440\\3379405994.py:87: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['text'] = df['text'].apply(stem_sentences)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy.engine import URL\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "train_original = pd.read_csv('./TrainingData/trainingdata2.csv')\n",
    "train_original.columns = ['target','id','date','flag','user','text']\n",
    "\n",
    "train=train_original[['id','text', 'target']]\n",
    "\n",
    "del train_original\n",
    " \n",
    "#region prepare stopwords list\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "#remove useful words from the stopword list\n",
    "sw.remove('not')\n",
    "sw.remove('no')\n",
    "sw.remove('nor')\n",
    "sw.remove(\"won't\")\n",
    "sw.remove(\"wouldn't\")\n",
    "sw.remove(\"shouldn't\")\n",
    "sw.remove(\"couldn't\")\n",
    "sw.remove('against')\n",
    "sw.remove(\"aren't\")\n",
    "sw.remove(\"haven't\")\n",
    "sw.remove(\"hasn't\")\n",
    "sw.remove(\"doesn't\")\n",
    "sw.remove(\"isn't\")\n",
    "#endregion\n",
    "\n",
    "def remove_pattern(text,pattern):\n",
    "    # re.findall() finds the pattern i.e @user and puts it in a list for further task\n",
    "    r = re.findall(pattern,text)\n",
    "    \n",
    "    # re.sub() removes @user from the sentences in the dataset\n",
    "    for i in r:\n",
    "        text = re.sub(i,\"\",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def stem_sentences(sentence):\n",
    "    #tokenize the sentence and remove the stems of the words\n",
    "    ps = PorterStemmer()\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [ps.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def preprocessTweet(df, sw):\n",
    "    #remove newlines\n",
    "    df['text'] = df['text'].str.replace(\"\\n\",\" \")\n",
    "    #turn all text to lowercase\n",
    "    df['text'] = df['text'].str.lower()\n",
    "    # remove twitter handles (@user)\n",
    "    df['text'] = np.vectorize(remove_pattern)(df['text'], \"@[\\w]*\")\n",
    "    #remove links\n",
    "    df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "    #remove special characters, numbers, punctuations\n",
    "    df['text'] = df['text'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "    #remove short words (length < 3)\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([w for w in x.split() if (len(w)>3 or w == 'no')]))\n",
    "    #remove duplicate tweets - bot prevention\n",
    "    df['text'] = df['text'].drop_duplicates(keep=False)\n",
    "    #remove quotes\n",
    "    df['text'] = df['text'].str.replace(\"quot\", \"\")\n",
    "    #remove NANs\n",
    "    df.dropna(inplace=True)\n",
    "    #remove stopwords\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (sw)]))\n",
    "    #remove empty tweets\n",
    "    df = df[df.text != '']\n",
    "    #stemming\n",
    "    df['text'] = df['text'].apply(stem_sentences)\n",
    "    return df\n",
    "\n",
    "train = preprocessTweet(train, sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6681540408435929\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(train['text'])\n",
    "df_bow = pd.DataFrame(bow.todense())\n",
    "\n",
    "train_bow = bow[:]\n",
    "train_bow.todense()\n",
    "\n",
    "x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(train_bow,train['target'],test_size=0.3,random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(x_train_bow, y_train_bow)\n",
    "y_pred = clf.predict(x_valid_bow)\n",
    "\n",
    "acc=accuracy_score(y_valid_bow,y_pred)\n",
    "print(acc)\n",
    "\n",
    "#save to csv\n",
    "resultsdf = pd.DataFrame({'text': x_valid_bow, 'pred Sentiment': y_pred})\n",
    "resultsdf.to_csv('results.csv', index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6677047707311107\n"
     ]
    }
   ],
   "source": [
    "tfidf = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(train['text'])\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "train_tfidf_matrix = tfidf_matrix[:]\n",
    "train_tfidf_matrix.todense()\n",
    "\n",
    "x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(train_tfidf_matrix,train['target'],test_size=0.3,random_state=42)\n",
    "\n",
    "dct = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "dct.fit(x_train_tfidf,y_train_tfidf)\n",
    "dct_tfidf = dct.predict(x_valid_tfidf)\n",
    "\n",
    "acc2=accuracy_score(y_valid_tfidf,dct_tfidf)\n",
    "print(acc2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86788393008fdb71dc79f29fdb41a6ff8baa293129381eaf3410cddb465d7ab5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
